---
title: "Three Models to Predict Major League Baseball Player Salary"
author: "Andrew Bates"
date: "December 20, 2018"
header-includes:
  - \renewcommand{\abstractname}{Executive Summary}
output: 
  bookdown::pdf_document2:
    toc: false
geometry: margin=1in
fontsize: 11pt
bibliography: ["references.bib"]
nocite: | 
 @broom, @R-car, @R-caret, @R-corrplot, @R-dplyr, @R-ggplot2, @Rglmnet, @R-MASS, @R-purrr, @R-randomForest, @R-recipes, @R-kableExtra, @R-here
abstract: "In this paper we compare the ability of three types of models to predict Major League Baseball player salary given performance metrics from the 1986 season and over a players career. We build and evaluate one model that falls under Leo Breiman's *data modeling* culture (linear regression), a model from his *algorithmic modeling* culture (random forest), and a model that straddles the two groups (lasso). Predictive power is compared via cross-validation error, error from predicting on a test set, and the increase in prediction error from cross-validation to test set. Random forest is the best predictor with a mean cross-validation error of 0.43 and a test set error of 0.46. Linear regression and lasso are comparable to each other with cross-validation errors of 0.53 and 0.54, respectively, and test set errors of 0.69 and 0.71. Both linear models had an approximately 30% increase in error going from cross-validation to test set predictions while random forest saw an increase of only 7%. In addition to predictive performance we compare the interpretability of each model. For linear regression this takes the form of the usual inferences on the regression coefficients. For lasso and random forest interpretability is viewed in the context of variable importance measures. The linear regression model included only one career level variable while the random forest deemed the career level covariates most important. The lasso was in between as it did not include all such variables but of those included, half were deemed most important."
---


# Introduction

In 2001 Leo Breiman described two approaches to analyzing data with statistical models [@breiman2001]. In *data modeling* we specify a stochastic model to describe the data, one that has known theoretical properties, and the main purpose is usually to make inferences on the population of interest. In the other approach, what Breiman calls *algorithmic modeling*, the focus is less on the the structure of the data itself and more about the output of the model. We are not concerned with finding a model that satisfies the theoretical assumptions needed to make inferences but are interested in whether the model can make accurate predictions on newly collected data. 


In this paper we compare Breiman's two modeling paradigms by analyzing Major League Baseball data with the goal of developing a model to predict a players salary. We examine one data model (linear regression), one algorithmic model (random forest), and one model at the intersection of the two approaches (LASSO). For each model, we discuss some advantages and disadvantages in terms of both predictive capability and interpretability.



# Methods

In this analysis we use the `Hitters` data from the R package `ISLR` [@R-ISLR], a companion package to *An Introduction to Statistical Learning with Applications in R* [@islrbook] containing the data sets used in the book. The Hitters data contains information on  322 players from the 1986 and 1987 Major League Baseball (MLB) seasons. There are 19 covariates included in this data set that can mostly be broken down into two categories: performance metrics for the 1986 season (number of at bats, number of home runs, etc.), and performance metrics based on a given players career (career runs, career hits, etc.). There are 16 continuous variables and three categorical variables, two of which indicate a players league (American or National) and division (East or West). For the 1987 season we have the players salary on opening day along with their league at the beginning of the season. 

The salary variable has 59 missing observations, 18% of the data. This was too many observations to ignore so we imputed the values using k-nearest neighboors before proceeding with the analysis. After examining histograms of the continuous variables, it was evident that transformations were in order. Salary, along with several covariates, were heavily right-skewed. We chose log transformations for these variables because it is a common technique and allows us to readily interpret linear regression coefficients. In all, 11 of the 20 variables were log transformed. All numeric variables were then subsequently centered about the mean and scaled by the standard deviation prior to model fitting.

Before fitting the models the data was split into a training and testing set with 20% reserved for testing. All three models were trained using 5-fold cross-validation with the final model chosen to be the one with the lowest root mean squared error (RMSE). In each cross-validation run for linear regression a stepwise procedure was used with Akaike Information Criteria (AIC) as the model selection criterion. Diagnostics were run on the model chosen via cross-validation and some covariates were subsequently ommited based on variance inflation factors and correlations between the covariates. A grid search was used for hyperparameter tuning of the lasso and random forest with 10 values considered for each. For the lasso the hyperparameter is the penalty on the $L_1$ norm of the regression coefficients and for random forest the hyperparameter is the number of randomly selected covariates considered at each split. A comparison of predictive ability for the three models was made through RMSE on the testing set as well as increase in prediction error going from cross-validation to test set. We investigate interpretability via coefficient interpretation for linear regression and variable importance for lasso and random forest.

The analysis was conducted using the statistical software R [@R-base]. This document was written using the R packages `R Markdown` [@R-rmarkdown], `knitr` [@R-knitr], and `bookdown` [@R-bookdown]. All materials used to conduct the analysis and compose the report can be found at https://github.com/asbates/stat696/tree/master/reports/baseball.



# Analysis

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, cache = TRUE)
library(here)
library(ISLR)
library(MASS) # load MASS first to prevent from masking dplyr::select
library(dplyr)
library(purrr)
library(recipes)
library(corrplot)
library(ggplot2)
library(caret)
library(broom)
library(car)
library(glmnet)
library(randomForest)
library(knitr)
library(kableExtra)
library(gridExtra)

theme_set(theme_bw())

data("Hitters")

bball_raw <- Hitters %>% 
  rename(
    at_bats = AtBat,
    hits = Hits,
    home_runs = HmRun,
    runs = Runs,
    rbis = RBI,
    walks = Walks,
    years = Years,
    career_at_bats = CAtBat,
    career_hits = CHits,
    career_home_runs = CHmRun,
    career_runs = CRuns,
    career_rbis = CRBI,
    career_walks = CWalks,
    league = League,
    division = Division,
    put_outs = PutOuts,
    assists = Assists,
    errors = Errors,
    salary = Salary,
    new_league = NewLeague
  ) %>% 
  as_tibble()

bball <- recipe(salary ~., data = bball_raw) %>% 
  step_knnimpute(salary) %>% 
  prep() %>% 
  bake(new_data = bball_raw)

mod_log <- function(x){
  ifelse(x > 0, log(x + 0.15), -log(-x + 0.15))
}

bball_logged <- bball %>% 
  mutate(
    log_salary = log(salary), 
    log_career_abs = log(career_at_bats), 
    log_career_hits = log(career_hits), 
    log_career_runs = log(career_runs) 
  ) %>% 
  mutate(
    log_home_runs = mod_log(home_runs), 
    log_career_hrs = mod_log(career_home_runs),
    log_career_rbis = mod_log(career_rbis), 
    log_career_walks = mod_log(career_walks), 
    log_put_outs = mod_log(put_outs), 
    log_assists = mod_log(assists), 
    log_errors = mod_log(errors) 
  )

bball_log_only <- bball_logged %>% 
  select(log_salary,
         at_bats,
         hits,
         log_home_runs,
         runs,
         rbis,
         walks,
         years,
         league,
         division,
         new_league,
         log_put_outs,
         log_assists,
         log_errors,
         log_career_abs,
         log_career_hits,
         log_career_hrs,
         log_career_runs,
         log_career_rbis,
         log_career_walks
         )


```



## Exploratory Analysis

There are a few areas of concern with the data used in this analysis. The first being a number of missing values for player salary[^2]. Almost 20% of the data has missing salary values. Most of these are likely due to retirement as 57 players retired in 1986[^3]. The others might be missing a salary because the players returned to the minor leagues which is not uncommon in baseball. Regardless of the reason, with so many missing values we decided to impute them using k-nearest neighboors. The data set is not particulary large at 322 observations so omitting missing values would be leaving out a large chunk of the data. 

Another potential issue with this data set is the correlations between the covariates. A plot of the correlation matrix for the continuous variables is given in the appendix (Figure \@ref(fig:corrplot-initial)). Several covariates have extremely high correlations, up to 98%. We also see groupings of covariates that have large correlations with each other. There are two groups of six variables each. One group contains career level variables and the other contains season level covariates. For example, the number of hits and at bats have a correlation of 97%. This should not be surprising as players can only get a hit if they are at bat. But the more troubling issue is that both have high correlations with number of runs (92% for hits and 91% for at bats) and the number of runs batted in (81% for hits and 82% for at bats). Additionally, they both have moderate correlations with number of home runs and number of walks. These are likely to present problems, especially in the linear regression model. However, the model selection procedure is based on prediction error and fits subsets of the covariates so it may not include an entire group of correlated variables. For now we refrain from removing any variables and reassess the issue after model selection.

Over half of the continuous variables exhibit skewness to some degree. This is of highest concern for the linear regression model but it may also affect the fit for lasso and random forest if most of the values are clumped together. Log transformations were used to account for skewness, favored for the interpretability of coefficients in the linear regression model. An example of this skewness is seen in Figure \@ref(fig:salary-hist) where we have a histogram of salary (left) along with a histogram of log salary (right). The log transformation clearly helps with skewness but also note that after transformation salary is approximately normally distributed. Histograms of the remaining variables are provided in the appendix (Figure \@ref(fig:histos-no-log)) for reference as well as histograms of the log transformed variables before and after transformation (Figures \@ref(fig:histos-with-log) and \@ref(fig:histos-with-log-career)). Also included are scatter plots of each covariate (or the log transformed version) versus log salary to assess the plausability of the linearity assumption for the linear regression model (Figures \@ref(fig:scatters-season) and \@ref(fig:scatters-career)). The plots show that each predictor variable has an approximately linear relationship with log salary.


```{r salary-hist, fig.width = 7, fig.height = 2, fig.cap="Histogram of player salary. The left plot is prior to transformation and the right plot is after log transforming."}
sal_hist <- 
  ggplot(bball, aes(salary)) +
  geom_histogram(bins = 30) +
  xlab("Salary")

sal_hist_log <-
  ggplot(bball_log_only, aes(log_salary)) +
  geom_histogram(bins = 30) +
  xlab("Log salary")

grid.arrange(sal_hist, sal_hist_log, ncol = 2) 
```



[^2]: None of the covariates had massing values.
[^3]: http://www.baseball-almanac.com/yearly/final.php?l=NL&y=1986




## Modeling Fitting


After splitting the data into a testing and training set, each type of model was fit to the training data set with 5-fold cross-validation. The same cross-validation training and testing sets were used for each type of model. In each case, the final model was the one that resulted in the lowest root mean squared error (RMSE), averaged over the cross-validation runs. Prior to performing cross validation each continuous predictor was first centered by the mean and scaled by the standard deviation.

For the linear regression model a backwards stepwise procedure was used on each cross validation run. The resulting fit was the one with the lowest Akaike Information Criteria and this was then used to predict on the cross-validation hold out set. The model with the lowest mean cross-validation error was then evaluated. The coefficient for log put outs had a reasonably large p-value at 0.13 but the p-values for the remaining coefficients were satisfactory. The real concern with this model is the variance inflation factors (VIF). Number of at bats had a VIF of 19.2 and number of hits had a VIF of 16.9. These are quite large and indicate collinearity issues. This should not be surprising as we noted earlier that these variables are highly correlated (97%) and they both have moderate to high correlations with other variables. Number of at bats generally has higher correlations with other covariates than number of hits so it was decided to remove at bats and keep number of hits.

After removing at bats from the linear regression model the collinearity issue was mostly abated. The largest VIF for this model was moderate at 5.9 (for number of runs batted in). Further diagnostics were done through plotting residuals versus fitted values and a QQ plot, both of which can be found in Figure \@ref(fig:lr-diagnostics) in the appendix. We see one concerning point with a rather large residual. The residual plot also shows a few points with variance a bit larger than most of the others. For the quantile-quantile plot, we see a point of concern that distances itself from the remaining points. However, recall that the primary purpose of this analysis is to compare predictive performance across each class of model and not necessarily to construct a model that perfectly satisfies its assumptions. Overall, both of the diagnostic plots in Figure \@ref(fig:lr-diagnostics) are reasonable approximations and are satisfactory for our purposes. Proceding with this model, cross-validation was then performed using the same fit on each run in order to get an estimate of out-of-sample RMSE and allow for comparison with the other models.


Lasso and random forest were fit with ten values of their respective hyperparameters on each cross-validation iteration. The parameter for the lasso is the penalty parameter on the $L_1$ norm of the coefficients. Ten equally spaced values were used ranging from 0.01 to 1. The optimal parameter was 0.01. For the random forest the hyperparameter is the number of randomly selected predictor variables considered at each split. Values ranged from 2 to 19 with the optimal parameter found to be three.


## Prediction Results

Table \@ref(tab:test-tab) displays the prediction error for each model across the cross-validation runs and on the testing set. As one would expect, the random forest model outperforms linear regression and lasso on both cross-validation error and test set error. The random forest is approximately 50% better than the alternatives in terms of relative error. The errors for linear regression and lasso are approximately the same which is a bit remarkable given that lasso often has improved predictive preformance compared to linear regression [@islrbook, p. 203]. Also of note, and perhaps most important, is the percent increase in error on the testing set compared to cross-validation error. Both linear regression and lasso saw an increase of about 30% while the random forest increase was only 7%. 


```{r test-tab}

# create train/test data
set.seed(42)
train_index <- createDataPartition(bball_log_only$log_salary,
                                   p = 0.8,
                                   list = FALSE)
training <- bball_log_only[train_index, ]
testing <- bball_log_only[-train_index, ]

# --- load the models ---
lr_step_small_trained <- readRDS(here("reports",
                                      "baseball",
                                      "results",
                                      "lr_step_small_trained.rds"))
lasso_trained <- readRDS(here("reports",
                              "baseball",
                              "results",
                              "lasso_trained.rds"))
rf_trained <- readRDS(here("reports",
                           "baseball",
                           "results",
                           "rf_trained.rds"))
# --- training predictions ---
lr_train_error <- lr_step_small_trained$results[1, 2]
lasso_train_error <- lasso_trained$results[1,3]
rf_train_error <- rf_trained$results[2,2]

# ---- test set predictions ----
# --- linear regression ----
lr_test_pred <- predict(lr_step_small_trained, newdata = testing)
lr_test_error <- postResample(pred = lr_test_pred,
                              obs = testing$log_salary)[1]
# ---- lasso ------
lasso_test_pred <- predict(lasso_trained, newdata = testing)
lasso_test_error <- postResample(pred = lasso_test_pred,
                                 obs = testing$log_salary)[1]
# ----- random forest -----
rf_test_pred <- predict(rf_trained, newdata = testing)
rf_test_error <- postResample(pred = rf_test_pred,
                              obs = testing$log_salary)[1]
pred_results <- tibble(
  Model = c("Linear Regression", "LASSO", "Random Forest"),
  `CV error` = c(lr_train_error, lasso_train_error, rf_train_error),
  `Test error` = c(lr_test_error, lasso_test_error, rf_test_error)
) %>% 
  mutate(
    `% Increase in error` = ((`Test error` - `CV error`) / `CV error`) * 100,
    `Test relative error` = `Test error` / min(`Test error`)
  ) %>%
  mutate(
    `% Increase in error` = formatC(signif(`% Increase in error`, 2),
                                    1, format = "f")
  ) %>% 
  select(Model,
         `CV error`,
         `Test error`,
         `% Increase in error`,
         `Test relative error`) %>% 
  arrange(`Test error`, `CV error`)

pred_results %>% 
  kable(digits = 2, caption = "Prediction results for linear regression, lasso, and random forest. Included are the mean cross-validation RMSE and test set RMSE. Also included is the increase in error from cross validation to testing and relative test set error.",
        align = c("l", "r", "r", "r", "r")) %>% 
  row_spec(0, bold = TRUE) %>% 
  kable_styling(latex_options = "hold_position")

```


Part of the performance gap between the linear methods and random forest is likely related to the groups of predictors with high correlations. Recall that there are two groups of six variables where each variable in a group has moderate to large correlation with the others. The optimal number of variables considered at each split for the random forest was three so it may be that only one or two predictors from each group is selected at a time. This would result in a better fit for each tree in the forest and hence a better fit when their predictions are averaged.

## Model Interpretation


Interpretation is another aspect for which the three models considered here differ. Even if the main objective for building a model is prediction accuracy one usually wants to have an understanding of how the model is working. However, the type of model dictates what kinds of interpretations we can make. For linear regression, inferences most often take the form of interpreting the regression coefficients. The lasso is based on a standard linear regression model but the nature of the fitting process makes linear-regression-like inferences difficult. Random forests are similarly difficult to interpret as we do in linear regression. Although there has been some recent development on inference for both lasso [@lasso-inference] and random forest [@rf-inference], they are beyond the scope of this analysis. Instead we take the traditional approach of model interpretation through feature importance measures.

A summary of the linear regression fit is given in Table \@ref(tab:lr-summary). Included are coefficient estimates of the model along with their associated standard errors, p-values, and 95% confidence intervals. Because log transformations and centering and scaling of the covariates was perfomed, the interpretations of the coefficients are not as straightforward as when no transformations are performed. There are three types of predictors in the model and each warrants a slightly different interpretation. The simplest case is the covariates that were not log transformed, such as number of hits. Since the predictors were centered and scaled prior to fitting, we can interpret number of hits as follows. Hits has a standard deviation of 46 and its coefficient estimate is 0.18. So every 46 hits a player gets per season is associated with a 100 * 0.18 or 18% increase in salary[^4]. Now let's consider a covariate that was log transformed, log home runs. Log home runs has a standard deviation of 1 and a coefficient of 0.11. Suppose log home runs for a given player increases by one standard deviation. If we take 1 + 1 and raise it to the power of 0.11 the result is 1.079. Subtract one from this and multiply by 100 and we get 7.9% which is the associated increase in salary we expect. The remaining type of predictor is the categorical variable for division. We leave this to the reader to interpret.


```{r lr-summary}

lr_step_small <- readRDS(here("reports",
                              "baseball",
                              "results",
                              "lr_step_small.rds"))
tidy_lr <- tidy(lr_step_small, conf.int = TRUE) %>% 
  mutate(
    Term = c("Intercept", "Hits", "Log home runs",
             "RBIs", "Walks", "Log put outs",
             "Log assists", "Log career RBIs", "Division: West"),
    Estimate = formatC(signif(estimate, 4), 2, format = "f"),
    SE = formatC(signif(std.error, 4), 2, format = "f"),
    `p-value` = formatC(signif(p.value, 4), 3, format = "f"),
    `95% CI` = paste("(",
                     formatC(signif(conf.low, 4), 2, format = "f"),
                     ", ",
                     formatC(signif(conf.high, 4), 2, format = "f"),
                     ")"
                     )
  ) %>% 
  select(Term, Estimate, SE, `p-value`, `95% CI`)

tidy_lr %>% 
  kable(align = c("l", "r", "r", "r", "r"), caption = "Summary for linear regression model of log salary against hits, log home runs, runs batted in, walks, log put outs, log assists, log career runs batted in, and division. Coefficient estimates are provided along with their standard errors, p-values, and 95\\% confidence intervals.") %>% 
  row_spec(0, bold = TRUE) %>% 
  kable_styling(latex_options = "hold_position")

```


We should note that these inferences do not take into account the model selection procedure and as such should be handled with caution. @sequential-regression discuss how to account for this with forward stepwise regression but we use backwards selection here. In a higher stakes setting one should consider the implications of the model selection mechanism but for reasons of simplicity we assume our inferences are reasonable approximations.

For lasso and random forest we interpret the models via variable importance measures. For variable importance of the lasso we use the absolute value of the regression coefficients. Feature importance for random forest is based on the difference in out-of-bag predictions before and after permuting a variable. Importance measures for both models are then scaled so that the most influential feature has an importance of 100. Plots of variable importance measures for lasso and random forest are given in Figure \@ref(fig:var-imp-plot). Only variables with a positive importance are plotted. For the lasso the most important feature is log career walks. This may be because if a player can get on base, regardless of how they do so, they at least have a chance to score. The top 6 features for random forest are career level variables which seems reasonable because they take into account a players performance over their career instead of just a single season. Sometimes in baseball a players performance in a given season does not reflect their performance over their career; they may play better or worse this season compared to the past five. The random forest seems to be taking this into account. In contrast, the linear regression model only included one career level variable, log career runs batted in. The liner regression model may be suffering in terms of predictive error because it did not include many career level variables. The lasso falls somewhere in between. Four career level covariates are included in the lasso model with two of them having the highest importance. Although we can not make the same type of statements about the lasso and random forest models that we can in linear regression, variable importance measures still give us insight into how the models are behaving.



```{r var-imp-plot, fig.width = 7, fig.height = 2, fig.cap = "Variable importance plots for lasso (left) and random forest (right). Importance measures are scaled so that the most influential feature has an importance value of 100. Only variables with importance larger than 0 are included."}

lasso_imp <- varImp(lasso_trained)$importance %>% 
  mutate(
    Term = c("At bats", "Hits", "Log home runs", "Runs",
             "RBIs", "Walks", "Years", "Log put outs",
             "Log assists", "Log errors", "Log career at bats",
             "Log career hits", "Log career home runs", "Log career runs",
             "Log career RBIs", "Log career walks", "League: National",
             "Division: West", "New league: National")
  ) %>% 
  filter(Overall > 0)

lasso_imp_plot <- ggplot(lasso_imp, 
                         aes(x = reorder(Term, Overall), y = Overall)) +
  geom_point() +
  geom_segment(aes(x = Term, xend = Term, y = 0, yend = Overall)) +
  labs(x = "",
       y = "Importance (lasso)") +
  theme(axis.title.y = element_text(size = 8)) + 
  coord_flip()


rf_imp <- varImp(rf_trained)$importance %>% 
  mutate(
    Term = c("At bats", "Hits", "Log home runs", "Runs",
             "RBIs", "Walks", "Years", "Log put outs",
             "Log assists", "Log errors", "Log career at bats",
             "Log career hits", "Log career home runs", "Log career runs",
             "Log career RBIs", "Log career walks", "League: National",
             "Division: West", "New league: National")
  ) %>% 
  filter(Overall > 0)

rf_imp_plot <- ggplot(rf_imp, 
                      aes(x = reorder(Term, Overall), y = Overall)) +
  geom_point() +
  geom_segment(aes(x = Term, xend = Term, y = 0, yend = Overall)) +
  labs(x = "",
       y = "Importance (random forest)") +
  theme(axis.title.y = element_text(size = 8)) + 
  coord_flip()

grid.arrange(lasso_imp_plot, rf_imp_plot, ncol = 2)

```



[^4]: Of course, interpretations for linear regression must be made with the caveat that only the variable under question changes and the others are held fixed.



# Conclusion

In this analysis we compared the ability of three classes of models to predict a baseball players salary given previous season information and overall career performance. The linear regression and lasso models achieved a comparable test set RMSE of 0.69 and 0.71, respectively. The random forest significantly outperfromed the other models with a test set RMSE of 0.46. Random forest also had less of a gap between its cross-validation and testing error compared to the other models. Linear regression and lasso had an approximately 30% increase in error while random forest had an increase of only 7%.

In terms of interpretability, the linear regression model has the advantage in that we can understand how a particular covariate relates to the response rather than just which covariates are important. That being said, we did not account for the linear regression model selection procedure so the inferences from the model are made with possibly strong assumptions. Although we did not make the same types of inferences for lasso and random forest, we nevertheless can obtain a high level understanding of the models through variable importance measures. There has been some recent results that allow for inferences in these models. They were beyond the scope of this analysis but perhaps a future study could investigate these methods. Also note that the transformations used in this analysis were fairly simple and the number of variables was not very large. Had we considered more complex transformations or had say, 30 predictors instead of 19, the interpretation of the linear regression model would more difficult.

It should be noted that this analysis has limitations. One issue is that the data is over 30 years old. Variables that have an impact on salary today are likely different than three decades ago. There have also been a substantial amount of new metrics introduced since 1986 that may have more of an impact on salary than the simple ones included in this data[^5]. It would be interesting to have a similar analysis performed on a contemporary data set to see if that is the case. The results might also have been different if a few more variables were included that would have been available in 1986. For example, player position could have allowed the models to treat pitchers different than outfielders. Alternative performance metrics may have also mitigated the issues with correlations in this data set. 


[^5]: See for example http://m.mlb.com/glossary/advanced-stats for a list of modern metrics.


\newpage

# (APPENDIX) Appendix A  {-}

# Supplementary Figures

```{r corrplot-initial, fig.width=6, fig.height = 3.5, fig.cap="Correlation matrix plot of continuous variables prior to transformations. Correlations are displayed as a percent."}
bball %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  corrplot(type = "lower",
           addCoef.col = "black",
           addCoefasPercent = TRUE,
           number.cex = 0.7,
           tl.srt = 10)
```


```{r corrplot-logged, fig.width = 6, fig.height = 3.5, fig.cap = "Correlation matrix plot of continuous variables after log transformations. Correlations are displayed as a percent."}
bball_log_only %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  corrplot(type = "lower",
           addCoef.col = "black",
           addCoefasPercent = TRUE,
           number.cex = 0.7,
           tl.srt = 10)
```


```{r histos-no-log, fig.width = 6, fig.height = 4, fig.cap = "Histograms of covariates for which log transformations were not performed"}

gghist <- function(data, var, bins = 30){
  var <- enquo(var)
  ggplot(data, aes(!!var)) +
    geom_histogram(bins = bins)
}


at_bats_hist <- gghist(bball, at_bats) + xlab("At bats")
hits_hist <- gghist(bball, hits) + xlab("Hits")
runs_hist <- gghist(bball, runs) + xlab("Runs")
rbis_hist <- gghist(bball, rbis) + xlab("Runs batted in")
walks_hist <- gghist(bball, walks) + xlab("Walks")
years_hist <- gghist(bball, years, bins = 20) + xlab("Years in major league")

grid.arrange(at_bats_hist,
             hits_hist,
             runs_hist,
             rbis_hist,
             walks_hist,
             years_hist,
             ncol = 2) 

```


```{r histos-with-log, fig.height = 7.5, fig.cap = "Histograms of season level covariates that were log transformed. The left side is the variable on the original scale and the right side is the variable on the log scale."}

hr_hist <- gghist(bball, home_runs)  + xlab("Home runs")
hr_log_hist <- gghist(bball_logged, log_home_runs, bins = 20) + 
  xlab("Log home runs")

po_hist <- gghist(bball, put_outs) + xlab("Put outs")
po_log_hist <- gghist(bball_logged, log_put_outs) + 
  xlab("Log put outs")

assist_hist <- gghist(bball, assists) + xlab("Assists")
assist_log_hist <- gghist(bball_logged, log_assists) +
  xlab("Log assists")
 
error_hist <- gghist(bball, errors) + xlab("Errors")
error_log_hist <- gghist(bball_logged, log_errors, bins = 20) +
  xlab("Log errors")

grid.arrange(hr_hist,
             hr_log_hist,
             po_hist,
             po_log_hist,
             assist_hist,
             assist_log_hist,
             error_hist,
             error_log_hist,
             ncol = 2)

```


```{r histos-with-log-career, fig.height = 7.5, fig.cap = "Histograms of career level covariates that were log transformed. The left side is the variable on the original scale and the right side is the variable on the log scale."}

cat_bats_hist <- gghist(bball, career_at_bats) + xlab("Career at bats")
cat_bats_log_hist <- gghist(bball_logged, log_career_abs) +
  xlab("Log career at bats")

chits_hist <- gghist(bball, career_hits) + xlab("Career hits")
chits_log_hist <- gghist(bball_logged, log_career_hits) + 
  xlab("Log career hits")

chr_hist <- gghist(bball, career_home_runs) + xlab("Career home runs")
chr_log_hist <- gghist(bball_logged, log_career_hrs) + 
  xlab("Log career home runs")

cruns_hist <- gghist(bball, career_runs) + xlab("Career runs")
cruns_log_hist <- gghist(bball_logged, log_career_runs) +
  xlab("Log career runs")

crbi_hist <- gghist(bball, career_rbis) + xlab("Career runs batted in")
crbi_log_hist <- gghist(bball_logged, log_career_rbis) +
  xlab("Log career runs batted in")

cwalk_hist <- gghist(bball, career_walks) + xlab("Career walks")
cwalk_log_hist <- gghist(bball_logged, log_career_walks, bins = 20) +
  xlab("Log career walks")

grid.arrange(cat_bats_hist,
             cat_bats_log_hist,
             chits_hist,
             chits_log_hist,
             cruns_hist,
             cruns_log_hist,
             crbi_hist,
             crbi_log_hist,
             cwalk_hist,
             cwalk_log_hist,
             ncol = 2)

```


```{r scatters-season, fig.height = 7.5, fig.cap = "Scatter plots of log salary vs. season level covariates. The blue line is a smoothing line."}

# function to produce scatter plots
ggscatter <- function(data, var){
  var <- enquo(var)
  ggplot(data, aes(x = !!var, y = log_salary)) +
    geom_point() +
    geom_smooth(method = "lm", formula = y ~ x, se = FALSE) +
    ylab("Log salary")
}


at_bat_scat <- ggscatter(bball_log_only, at_bats) + xlab("At bats")
hits_scat <- ggscatter(bball_log_only, hits) + xlab("Hits")
hrs_scat <- ggscatter(bball_log_only, log_home_runs) + xlab("Log home runs")
runs_scat <- ggscatter(bball_log_only, runs) + xlab("Runs")
rbis_scat <- ggscatter(bball_log_only, rbis) + xlab("Runs batted in")
walks_scat <- ggscatter(bball_log_only, walks) + xlab("Walks")
years_scat <- ggscatter(bball_log_only, years) + xlab("Years")
po_scat <- ggscatter(bball_log_only, log_put_outs) + xlab("Log put outs")
assists_scat <- ggscatter(bball_log_only, log_assists) + xlab("Log assists")
errors_scat <- ggscatter(bball_log_only, log_errors) + xlab("Log errors")

grid.arrange(at_bat_scat,
             hits_scat,
             hrs_scat,
             runs_scat,
             rbis_scat,
             walks_scat,
             years_scat,
             po_scat,
             assists_scat,
             errors_scat,
             ncol = 2)

```


```{r scatters-career, fig.height = 7.5, fig.cap = "Scatter plots of log salary vs. career level covariates. The blue line is a smoothing line."}

cat_bat_scat <- ggscatter(bball_log_only, log_career_abs) + 
  xlab("Log career at bats")
chits_scat <- ggscatter(bball_log_only, log_career_hits) +
  xlab("Log career hits")
chrs_scat <- ggscatter(bball_log_only, log_career_hrs) +
  xlab("Log career home runs")
cruns_scat <- ggscatter(bball_log_only, log_career_runs) +
  xlab("Log career runs")
crbis_scat <- ggscatter(bball_log_only, log_career_rbis) +
  xlab("Log career runs batted in")
cwalks_scat <- ggscatter(bball_log_only, log_career_walks) +
  xlab("Log career walks")

grid.arrange(cat_bat_scat,
             chits_scat,
             chrs_scat,
             cruns_scat,
             crbis_scat,
             cwalks_scat,
             ncol = 2)

```


```{r categorical-plots, fig.height = 7.5, fig.cap = "Bar plots of categorical variables."}
league_bar <- ggplot(bball, aes(x = league)) +
  geom_bar() +
  scale_x_discrete("League at end of 1986 season",
                   labels = c("American", "National"))

division_bar <- ggplot(bball, aes(x = division)) +
  geom_bar() +
  scale_x_discrete("Division", labels = c("East", "West"))

new_league_bar <- ggplot(bball, aes(x = new_league)) +
  geom_bar() +
  scale_x_discrete("League at start of 1987 season",
                   labels = c("American", "National"))

grid.arrange(league_bar,
             division_bar,
             new_league_bar,
             ncol = 1)
```



```{r lr-diagnostics, fig.width = 7, fig.height = 3, fig.cap = "Diagnostic plots for the linear regression model. The left hand plot is residuals vs. fitted values. The right hand plot is a QQ plot of the residuals vs. normal quantiles. The blue lines are reference lines indicating zero (left) and the y=x line (right)."}

lr_step_small <- readRDS(here("reports",
                              "baseball",
                              "results",
                              "lr_step_small.rds"))

lr_aug <- augment(lr_step_small)

# residuals vs. fitted values
resid_fit <- ggplot(lr_aug, aes(x = .fitted, y = .resid)) +
  geom_point() + 
  geom_hline(yintercept = 0, color = "blue") +
  labs(x = "Fitted values", y = "Residuals")

# qq plot
qq <- ggplot(lr_aug, aes(sample = .resid)) +
  stat_qq(distribution = qnorm) +
  stat_qq_line(distribution = qnorm, color = "blue") +
  labs(x = "Theoretical quantiles", y = "Sample quantiles")

grid.arrange(resid_fit, qq, ncol = 2)

```



# R Code


<!--
\begin{appendices}

\section{Supplementary Figures}

\section{Model Stuff}

\end{appendices}
-->

\newpage

# References
